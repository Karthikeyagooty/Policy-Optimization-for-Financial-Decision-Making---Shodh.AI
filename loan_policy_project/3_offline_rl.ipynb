{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_utils import load_data, select_and_clean, encode_and_split, build_rl_dataset\n",
    "\n",
    "DATA_PATH = './data/accepted_2007_to_2018.csv'\n",
    "df = load_data(DATA_PATH, nrows=200000)\n",
    "proc = select_and_clean(df)\n",
    "# choose features used for state\n",
    "features = [c for c in proc.columns if c != 'target']\n",
    "states, actions, rewards = build_rl_dataset(proc, features=features)\n",
    "print('States shape:', states.shape)\n",
    "print('Actions shape:', actions.shape)\n",
    "print('Rewards shape:', rewards.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96d2a49",
   "metadata": {},
   "source": [
    "## Behavior Cloning (policy-learning via supervised learning)\n",
    "Train a classifier to predict the action taken in the dataset (here action==1 for approved). This produces a policy pi(a|s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b061e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(states, actions, test_size=0.2, random_state=0, stratify=actions)\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train, y_train)\n",
    "print('BC accuracy:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd107ad",
   "metadata": {},
   "source": [
    "### Policy Evaluation (Estimated Policy Value)\n",
    "A simple importance-sampling OPE is not directly possible because we do not have behavior policy probabilities. As a crude estimate, we can compute the expected reward of a deterministic policy on the dataset by averaging rewards where the policy's action matches the dataset action, and otherwise applying the counterfactual reward estimate (here we only have observed outcomes when action==1).\n",
    "This notebook provides a basic conservative estimate: average reward on states where policy approves and dataset approved; treat other approvals as unknown. For a production-quality evaluation use dedicated OPE libraries."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
